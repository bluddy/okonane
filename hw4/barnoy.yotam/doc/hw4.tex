% This template was created by Ben Mitchell
% for the JHU AI class, CS 335/435, spring 2008. 

% Updated spring 2009 by Ben Mitchell.

% For those who want to learn LaTeX, this is a decent place to start:
% http://en.wikibooks.org/wiki/LaTeX
% Note that the proper pronunciation is "la tek", not "lay teks".
%
% Other references are linked from Mr. Mitchell's JHU CS web page, at
% www.cs.jhu.edu/~ben/latex/
%
% There are lots of latex tutorials and primers online; just be careful with
% google images.




% the documentclass line says that this is an "article" (as opposed to, eg. a
% book or a report).  This defines the basic formatting of the document.  The
% arguments say that we want 12 point font (the default is 10), and 8.5"x11"
% paper (the default is A4)
%
% If we said "report" instead of "article", then the title would be put on a
% separate title page, rather than at the top of the first page of text, and the
% numbering would be set up expecting chapters to be the top level divisions,
% above sections.  Try it out, but be sure to use "article" for your
% submission.
\documentclass[12pt, letterpaper]{article}

% the usepackage line states what extra packages we want to use
% we use several "AMS" packages, which are created and distributed
% by the American Mathematical Society (AMS), and have a number of useful
% macros for writing math and equations.
%
% the graphicx package is one of several packages that can be used for
% including images in a LaTeX document
\usepackage{amsmath, amsthm, graphicx,url}
%\usepackage[natlib=true]{biblatex}
%\bibliography{hw4bib}


% the title should contain your title.  Note the "\\"; this causes a linebreak.
% By default, LaTeX ignores extra whitespace, including single linebreaks.
% If you leave a blank line inbetween two blocks of text, that is interpreted as
% a paragraph break.
\title{Genetic and ID3 Decision Trees:\\ A functional approach}

% Put your name in the author field
\author{Yotam Barnoy}

% this begins the actual text of the document; everything before this is
% refered to as "preamble"
\begin{document}

% the maketitle command formats and inserts the title, author name, and date
\maketitle

\begin{abstract}
Decision trees are a widely used tool in Machine Learning, as are genetic algorithms. We compare the accuracy and performance of two methods of producing decision trees: the well known ID3 algorithm, and genetic algorithms. We also compare various parameters that the genetic algorithm approach uses. We implement both methods using OCaml -- a functional language. We conclude that both algorithms are suited for different purposes, with ID3 being sufficient for simple data sets.

\end{abstract}

\section{Introduction}
Decision Trees have found uses in a wide range of fields, from car-manual repair instructions to operations research. Much of their value comes from their simplicity --- at each node in the decision tree, several options are presented in the form of the node's children, and so, starting from the complete space of the decision tree and descending to a particular sub-set represented by the leaf involves only evaluating simple comparison operations and following the node ordering.

Within the field of Machine Learning, decision trees are a widely used method for classification\cite{tomMitchellML}. Constructing optimal decision trees, however, is not a simple problem to solve\cite{decisionTreesNPComplete}. A commonly used heuristic is to try and make the decision tree as small as possible, while still attaining a good measure of accuracy. This heuristic, which follows the principle of Occam's Razor, was formalized by Rissanen\cite{rissanen1978modeling}, and it is at the heart of the Quinlan's ID3 algorithm\cite{quinlan1986induction}.

Many approaches have been suggested for the creation of optimal decision trees. \cite{safavian1991survey} presents a thorough survey of approaches to the subject. In this paper, we focus on two primary approaches: the ID3 algorithm, and genetic algorithms.

Genetic algorithms are --- much like Artificial Neural Networks --- inspired by a natural process, specically, evolution. This idea originated in the 1950s and 1960s, when Rechenberg(1965) introduced the concept of ``evolution strategies''\cite{melanie1999introduction}. It was made concrete by John Holland, who invented ``Genetic Algorithms'', per se\cite{holland197388}. Similarly to the way that Natural Selection selects from a group of organisms the sub-group that is most `fit', with fitness being a relative term, so too, Genetic Algorithms select from a set of solutions the ones most `fit', according to a certain measure of fitness known as the fitness function. Over time, a selective pressure causes the population to conform to the requirements of the fitness function, which is usually tuned to encourage a proper solution to the given problem.

The aforementioned ID3\cite{quinlan1986induction} algorithm uses a greedy strategy to attempt to create the smallest tree possible on an iterative basis. At each split of the tree, the best attribute for the next node is selected, until either all data is consumed or no more splitting of the tree is possible.

This paper compares the performance of the ID3 algorithm to a genetic algorithm approach to creating decision trees. Since genetic algorithms require many parameters to work, a comparison is also made between different genetic algorithm parameters and how they affect their results. While these algorithms aren't novel, we shall attempt to study the effects of modifying different parameters, and how the different algorithms stack up when measured on different data sets. 

For our experiments, the implementation of both algorithms was carried out in Ocaml, which is an heir to a long line of functional languages, starting with Lisp\cite{hudak1989conception}. Functional Languages lack the implicit state that imperative languages possess, requiring it to be strung along explicityly. They are declarative in nature, representing a somewhat modified form of the lambda calculus, as formulated by Alonzo Church. The big advantage of functional programming over imperative programming is the limitation of mutable state, which may be seen as a confounding factor in writing code. The disadvantages include performance challenges and debugging facilities, which will be addressed in this paper.

Section 2 of this paper details the theoretical underpinnings for the algorithm. Section 3 describes the challenges of using a functional language to implement decision try algorithms and how they were addressed. Section 4 details the algorithm and our experiments, while section 5 lists the results of the experiments. Finally, section 6 analyzes the results and suggests possible future work in the field.

\section{Learning Decision Trees}
Like many supervised learning algorithms, decision trees are learned using a training phase followed by a test, or classification phase. Classical decision tree methods can only be applied to discrete-valued problems: problems where input attributes have a finite number of discrete values. 

Consider a data set $D$ with $n$ attributes $A_i$ ranging from $A_1, A_2, A_3 \dots A_n$. Each attribute can have possible values $v_{ij}$, with $j$ ranging from $1\dots m_i$ where $m_i = |A_i|$. A vector $U$ contains an ordered sequence of values $v_{ij}$ as in $v_{1j}, v_{2j}\dots v_{nj}$. Additionally, if the data can be classified into $c$ classes, a labeled vector contains a label $l_k$ in the range $l_1\dots l_c$. 

The task of a classification algorithm is to find patterns in a set of labeled vectors (one containing a label), and then proceed to assign labels to unlabeled vectors. The first part of this process is referred to as training phase and the second as testing, or classification phase.

Decision trees codify a certain pattern in the data. As piece-wise functions with discrete inputs, they represent an arbitrarily complex conditional expression over the values of the data. 

Practically speaking, implementing decision trees usually involves using whatever method the programming language possesses to describe a tree data structure. In C, this will involve pointers and structures, whereas a more object-oriented language will use a class-based approach. In OCaml, as in other functional languages, the natural method to describe trees is with ADTs, or \emph{Abstract Data Types}. We will examine this in more detail in section 3. Regardless, every member of a tree $T$ can be either an internal node or a leaf. An internal node of $T$ consists of an attribute $A_i$ possessing exactly $m_i$ children. In fact, this is the main constraint of the decision tree structure, and it must not be violated or the tree is not a proper decision tree. Each child corresponds to a value $v_{ij}$ of the attribute $A_i$, and each child can be either another internal node or a leaf. Leaf nodes consist only of the value $v_{ij}$ they represent, and the label attached to them, $l_k$.

After building a tree in the training phase, the way a label is chosen for a specific test vector is by following the nodes in the tree from the root of the tree down until a leaf node is reached. At every juncture in the tree, a value $v_ij$ is chosen corresponding to the value of attribute $A_i$ in the test vector. The choice determines the path to take. Once a leaf is reached, it serves as the predicted label for that test vector.

\subsection{Traditional Methods}
The traditional method of building decision trees is a greedy algorithm known as ID3 (Iterative Dichotomiser 3)\cite{quinlan1986induction}. This algorithm consists of the following steps:

Given data $D$ consisting of labeled vectors as described above, and a list of remaining attributes $A_1\dots A_n$:

\begin{enumerate}
    \item If there are no attributes left to split on, form a leaf with the majority label among the remaining vectors.
    \item If the vectors contain only one label class $l_k$, form a leaf with that label.
    \item Otherwise, find the best remaining attribute $A_i$ on which to split the data. Create a node for $A_i$ and children for every value $v_{ij}$.
    \item Iterate over each child, passing only the vectors that contain $v_{ij}$ and a reduced list of remaining attributes.
\end{enumerate}

This algorithm is greedy since it cannot reason about the future: it cannot choose to split over a weaker attribute so that it may split more effectively later on.

Of course, the tricky part is finding a heuristic with which to decide which node is the best node to split on. This is the real insight of ID3.

% note the '*' character; this causes  a (sub)section to not be numbered.
\subsubsection*{Entropy and Information Gain}

ID3 uses a concept called entropy to choose the best attribute on which to split. Entropy in the mathematical sense refers to the \emph{Shannon entropy}, and is a borrowed term from information theory\cite{entropyWikipedia}, where it refers to the amount of uncertainty in a random variable as expressed in bits. The idea is that a purely deterministic signal contains no information: if I know that a random variable will always be $1$, then I don't need to convey anything about that variable. In this case, the variable has an entropy of $0$. On the other end of the scale is a 50-50 chance of having either $0$ or $1$. In this case, if we had to guess the value of the random variable, we'd have no information to go by. We would have to have 1 bit of information to let us know if $0$ or $1$ came up. In general, the more fair a random variable is (i.e. the more uniformly distributed), the greater the entropy, and the less predictable the variable is. The more skewed the probability distribution is, the lower the entropy, since we can better guess at the outcome of events. 

The way this relates to decision trees is through a concept known as information gain. When we start out with our data, that data has a certain entropy to it, based on the relative frequency of the labels in the data. If the labels are uniformly distributed, the entropy of the data is high, but if some label classes are much more common than others, the entropy is lower -- we can guess that the majority labels will come up more often and we'd be right, assuming a similar distribution in the test and training data. The goal of a decision tree is to lower the entropy (uncertainty) of the data as much as possible, and the notion of lowering entropy is known as information gain. 

Entropy is defined mathematically as 
\begin{equation}
    H(X) = - \sum_i P(x_i)\log_2 P(x_i) = - \sum_i \frac{n_i}{N} \log_2 \frac{n_i}{N}
\end{equation}

This means that for a given set of vectors, we iterate over every subset, taking the relative proportion of the particular subset of the data, taking the logarithm of that amount, and then scaling it by the same relative proportion. 

Applying this to decision trees, we get the following:
\begin{equation}
    H(D) = - \sum_k \frac{|\{y_k \in D\}|}{|D|} \log_2 \frac{|\{y_k \in D\}|}{|D|}
\end{equation}

That is, we sum over the log of the relative frequencies of labels, multiplied by those relative frequencies.

To evaluate the information gain from choosing a particular attribute for the decision tree, we need the difference in entropy before splitting the data on the particular attribute and the post-split entropy:
\begin{equation}
    IG(D,A_i) = H(D) - \sum_{v_{ij}} \frac{|\{\boldsymbol{x}\in D|x_{ij} = v_{ij}\}|}{|D|} \cdot H(\{\boldsymbol{x}\in D|x_{ij} = v_{ij}\})
\end{equation}

In other words, after we split the data, the label frequencies become distributed among the different attribute values for $A_i$, so we need to calculate all of those partial entropies and then weight them according to their to the relative frequencies of the values $v_{ij}$ in the data. This gives us the new entropy. Subtracting the new value from the old entropy gives us the information gain. If we try to maximize the information gain, i.e. we try to split on every attribute and choose the one that gives us the most information gain, we move the decision tree to the lowest state of entropy. Ideally, we'd like to categorize every label by splitting on an attribute --- every attribute should have only one kind of label. This would reduce our entropy to 0. The sooner we can get to minimal entropy, the more effective our decision tree. Of course, this is all assuming that the training data has the same distribution as the test data, which is not necessarily true.

One issue that comes up with the information gain heuristic is that it doesn't work well for attributes $a_i$ where there are many values $v_{ij}$. The reason for this is that when you have an attribute with many possible values, those values tend to split up the data set into very small, uniform subsets. However, splitting the training data up into such small pieces is too specific. For example, if a unique name is attached to every vector in the training set, that attribute would score extremely well on the information gain heuristic, but it probably wouldn't do well with any other data. This is the problem of overfitting the test data.

To compensate for problem sets that have attributes with many values, we can use a different heuristic known as information gain ratio (IG). This measure divides the information gain by the intrinsic value (IV) of the attribute, which consists of the entropy of the attribute with respect to its values:
\begin{equation}
    IV(D, a_i) = - \sum_k \frac{|\{\boldsymbol{x} \in D\}|{x_{ij} = v_{ij}}}{|D|} \log_2 \frac{|\{\boldsymbol{x} \in D\}|{x_{ij} = v_{ij}}}{|D|}
\end{equation}

Dividing the information gain by the intrinsic value of the attribute, we get
\begin{equation}
    IV(D, a_i) = \frac{IG(D, a_i)}{IV(D, a_i)}
\end{equation}

The reason this works is that the entropy over the attribute values grows for a uniformly distributed attribute. The more subdivisions splitting over the attribute values produces, the higher the intrinsic value of the attribute. This neutralizes some of the high score produced by the information gain.

Thus, for data sets containing attributes with many values, we can use the information gain ratio rather than the information gain. But this is not the whole story. Even with information gain ratio as our measure, we can overfit the training data. This is because the ID3 algorithm will keep adding nodes to our tree so long as an attribute is available to split on, and so long as we have labels to split. This means that even weak attributes that do not contribute much to lowering the entropy of the tree will make their way in -- usually as the last attributes to be added. We would like to block these low-value attributes from being added to the tree, as they could mess up classification of the test data. The way we do this is with pruning.

There are several techniques for pruning. Among them is the Chi-Square method, which given a measure of deviance and a \emph{dof} or degrees or freedom value, will give the statistical likelihood of rejecting the null hypothesis that the attribute does not correlate with the previous distribution within the data. 

We modify the ID3 algorithm to include this component: after evaluating the best available attribute on which to split and before actually splitting, we calculate a deviance measure of the data based on the hypothetical attribute choice. Considering each label class at a time and the data coming into a particuar node, we refer to the vectors with the given label as the `positive' examples, and to all other vectors as the `negative' examples. We then calculate the expected numbers of positive and negative examples.
\begin{eqnarray}
    \hat{p} = \frac{p_{in}}{p_{in}+n_{in}} \cdot |D|\\
    \hat{n} = \frac{n_{in}}{p_{in}+n_{in}} \cdot |D|
\end{eqnarray}
where $D$ is the data going \emph{out} of the node after splitting on the chosen attribute. We then calculate the deviance:
\begin{equation}
    dev(A_i) = \frac{(\hat{p} - p_{out})^2}{\hat{p}} + \frac{(\hat{n} - n_{out})^2}{\hat{n}}
\end{equation}
where $p_{out}$ and $n_{out}$ are the true numbers of positive and negative examples after dividing the data by the attribute. Feeding this measure to the chi-square function with degree of freedom equal to $(|C| - 1)(|V_i| - 1) $ ($|C|$ being the number of classes and $|V_i|$ being the number of values of the chosen attribute gives use the probability of rejecting the null hypothesis. Generally speaking, $5\%$ is considered a good statistical value to decide by. So we modify our algorithm to winnow out any attribute nodes for which we cannot disprove the notion that they don't contribute to our data.

\subsection{Genetic Algorithms}
Genetic algorithms work by creating an initial set of possible solutions to a problem, and then mixing and mutating those solutions over time, while choosing the better solutions with a stochastic process. \cite{melanie1999introduction} provides an excellent overview of the stages of a genetic algorithm:

\begin{enumerate}
    \item Starting with a randomly generated set of $n$ solutions,
    \item Calculate the \emph{fitness} of each solution
    \item Using a \emph{selection function}, choose a pair of `parent solutions'. It's important to search with replacement i.e. to allow parents to be chosen again.
    \item A \emph{crossover probability} $p_c$ determines whether the parents are mutated with each other to form a child pair, or whether they are left intact and proceed to the next generation. \emph{Crossover} is a genetic term for pieces of DNA switching sites between parents, and it's an operation that needs to be supported by the \emph{encoding} of the solutions.
    \item The parents which produced children are subject to a certain \emph{replacement policy}, whether it be discarding the parents (called simply \emph{replacement}), joining them with the rest of the new generation (called \emph{elitism}) or some other policy.
    \item The new population is mutated at random using mutation probability $p_m$. 
    \item Discard the old population and keep the new one. Proceed to step 2.
\end{enumerate}

The first requirement for genetic algorithms is to find some form of encoding that allows manipulations with the genetic operators (crossover, mutation, fitness function), all while maintaining a valid individual solution to the problem. The next requirements involve filling in all the missing parameters of the algorithm. 

A fitness function should cause selective pressure towards solving the problem, but finding an adequate function that leads towards a solution is often tricky. 

A replacement-based replacement policy discards all the old parents, which can throw out some potentially good solutions, while an elitism-based policy chooses the best of all current trees, which could cause early convergence to an especially fit solution rather than a proper exploration of the search-space.

A selection function/policy determines which solutions are chosen and at what rate. A common-used selection function is \emph{fitness proportionate selection}, which is essentially a roulette-wheel function where each solution is given slots on the wheel based on the ratio of the fitness of the particular solution and the total fitness of every solution combined. Another popular method is using \emph{rank-based selection}, which ranks the solutions according to fitness, and then allocates spots on the roulette wheel based on rank rather than fitness proportion. This means that even solutions with relatively low fitness can still get selected. Finally, \emph{tournament selection} randomly picks a certain number of solutions and then carries out rank selection on that set.

Mutation probabilities are usually very low, in the order of 1\%, since mutation can cause serious drift away from the fitness function. On the other hand, mutation also helps avoid converging to a local maximum in the solution space.

\subsubsection*{Genetic Algorithms for Decision Trees}

When applying genetic algorithms to decision trees, a natural choice of encoding is the tree itself. The problem with converting to any other encoding is how to maintain tree correctness once the genetic operators are executed on the trees. Specifically, decision trees must maintain the right number of decision nodes for a given attribute. Using the tree itself as a solution encoding makes the most sense, since construction of the tree enforces its correctness constraints.

After deciding on a proper encoding, several other choices must be made. A fitness function needs to be chosen to serve as the selective pressure on the population. In our case, we did not select a fitness function until experimentation, but we chose a set of three possible fitness functions: precision, recall, and a mix of precision and recall. Given a set of test data $D$ with positive examples (ie. labels in a chosen class $C$), negative examples (labels in other classes), and a classification result with positive predicted labels of the chosen class and predicted labels of other classes, the following table describes the true positive, true negative, false positive and false negative counts:

\begin{center}\begin{tabular}{|c|c|c|c|}
    \hline
    & & \multicolumn{2}{|c|}{Real values} \\
    \cline{3-4}
           & & Positive & Negative \\ \hline
    Classified & Positive& $T_p$ & $F_n$ \\ \cline{2-4}
    values & Negative& $F_p$ & $T_n$ \\ \hline
\end{tabular}\end{center}

The precision and recall functions are defined as
\begin{eqnarray}
    Precision = \frac{T_p}{T_p + F_p}\\
    Recall = \frac{T_p}{T_p + F_n}
\end{eqnarray}

Not knowing which of these will be more effective (or a combination of the 2), we implemented all 3 options and tested them out in our experiments. Each of these measures promotes minimizing the errors in the data. We also added a small component to encourage smaller trees (similar to the heuristic used for ID3) by multiplying the tree size by a constant and subtracting that from the fitness of the tree.

The next important choice is that of the mutation and crossover genetic operators. For crossover,we select a node at random out of each tree and swapped the nodes between the trees. This turned out to be an expensive operation, and we used zippers to do it more efficiently (see next section). For mutation, we chose a random subset of the following three operations: 

\begin{enumerate}
    \item Changing the attribute of a given node. This can only be done between attributes that have the same number of values, or you can violate the decision tree constraint.
    \item Adding an attribute to a tree at a leaf node, turning that leaf itself into a node. Adding in the middle of the tree is problematic because it's not clear how to populate the subnodes of the new attribute node.
    \item Removing a node, deleting the sub-tree beneath it.
\end{enumerate}

We implemented a variety of selection functions: fitness-proportional, rank-based and tournament-based. Rather than deciding up front which one to go with, we used our experiments to find the one that worked best. This applies to other parameters used in the algorithm: replacement function, mutation probability, etc. Rather than decide on values beforehand, we experimented and found the results that seemed to do best. Note that this was by no means a rigorous, statistical selection: we simply chose the parameters that seemed to help the genetic algorithm the most.

\section{Functional Programming}
Functional programming languages such as OCaml and Haskell are a variation on lambda calculus. Construction a decision tree with functional programming is fairly trivial since most functional languages use Abstract Data Types, which are well suited for trees. Travelling around in the tree involves simply iterating over the nodes and pattern matching on the node attributes and values. Our decision tree representation looked as follows:

\begin{verbatim}
type attrib_t = int
type val_t = Node of string * tree_t
           | Leaf of string * label_t
and tree_t = attrib_t * val_t list}
\end{verbatim}

In OCaml, most data types are referentially transparent, which means that one is discouraged form obtaining a pointer and directly modify a node. Data structures are immutable by default, meaning that trees cannot be modified in-place. This, as well as the fact that global pointers (refs) and global state is discouraged, means that functions can often be tested individually, often from the OCaml interpreter's REPL command line. 

At the same time, there are costs to using functional languages. Since data structures are created rather than modified, and since allocation is one of the greatest costs in modern computer systems, functional languages often suffer in their performance. Fortuonately, functional data structures exist that mitigate some of these data modification costs by reusing parts of the old data strucutures and allocating only the changing ones. For example, mutation of our trees required creation of the new mutated nodes, but the old nodes below the mutation point could be reused. 

Another issue we discovered while implementing the genetic algorithm is the cost of modifying a particular node in a tree, particularly when performing crossover. Crossover involves finding two attribute nodes in two parent trees and swapping them. This would seem to be a simple operation, involving copying the trees and then pointer swapping in imperative languages. In functional languages, however, this is an expensive operation. There is no default way to refer to a node in a tree directly and to `implant' that node back in a tree. In order to refer to a node, an integer must be used to refer to the position of the node in some linearization (i.e. traversal) of the tree. The nodes need to be retrieved from the trees by traversing the trees, and then 2 more traversals are needed to recreate the trees. Fortunately, \cite{huet1997functional} invented the `zipper', which allows constant-time traversal of a functional data structure. 

A Zipper is a list structure describing a path from the position in the tree to the root of the tree. Our zipper is defined as follows:
\begin{verbatim}
type loc_t = Top 
        | Path of loc_t * attrib_t * string * val_t list * val_t list
type zipper_t = loc_t * tree_t
\end{verbatim}

The first part of the zipper contains the location in the tree using a path loc type, that is either Top (top of the tree) or a Path in the tree, containing the next loc, the attribute and node value of the particular node we're looking (for reconstruction), a list of values left of the current node, and a list of values right of the current node. The second element of the zipper tuple contains the tree below the current node. Since the zipper's path contains lists of values, which themselves contain trees, the zipper is essentially a path to the root with the untaken branches glued along the way.  Using the zipper we can move up, down, left or right in the tree in constant time. We made use of the zipper to get good performance, which was needed for the genetic algorithms.


\section{Algorithms and Experimental Methods}

We had several ways to judge the performance of the different algorithms. Our program spits out accuracy, precision and recall statistics per fold, as well as a confusion matrix. We settled on accuracy as the primary method of comparing different runs of the algorithms. Ultimately, accuracy is what one cares about most when classifying data. Certainly a plane manufacturer would have a different opinion on the matter when it comes to testing his planes, but for us, accuracy was good enough. The other data was useful for additional analysis, but it wasn't our deciding factor. 

Every experiment involved 8-fold cross-validation, i.e. we divided the data set into 8 folds or chunks, and then iterated through the fold combinations, choosing one to test on and the rest with which to train. We initially tried 10-fold cross-validation but that seemed like it was driving the example counts too far down per fold, so we scaled back to 8-fold cross-validation.

K-fold cross-validation is a good way to get a lot of use out of a limited amount of data. By iterating and using different sub-sets of the data for testing and training data, we can get many different results, allowing us to have statistically meaningful data. K-fold cross-validation is much more useful than 2-fold cross-validation (holdout), but it isn't as useful as Leave-one-out cross-validation, which just isn't realistic given the training time of especially the genetic algorithms. Given our constraints, k-fold cross-validation seemed like the right way to go.

For all experiments, we took the mean and standard deviation of all runs. However, we performed statistical analysis (a T-Test) only for one experiment --- the comparison between ID3 and GAs. The rest was done by estimation.

We performed several sets of experiments. The default parameters for genetic algorithms were set to a population of 100, with 400 generations, a precision fitness function, full replacement policy, fitness-proportionate selection, with a mutation probabilty of $1\%$ and a crossover probability of $40\%$. We then tested different combinations:

\begin{enumerate}
    \item We compared different fitness functions: recall, precision and a mixed function, keeping all other parameters equal. We used the winning parameter for our comparison against ID3.
    \item We compared different filter probabilities: if each generation trains on a random subset of the data, how does that affect accuracy? How does it affect the processing time?
    \item We compared different selection criteria and their effect on accuracy.
    \item We compared different replacement functions and their effect on accuracy.
    \item We compared the accuracies of different ID3 parameters: using information gain, information gain ratio, and information gain with chi-square pruning, and information gain ratio with chi-square pruning.
    \item We compared the best version of genetic algorithms to the best version of ID3 and ran a statistical T-test on the results. We also compared execution times.
\end{enumerate}

In this section, you describe any details of your algorithm that you left out of
the more theoretical discussion in the previous section.  You don't need to list
every single parameter value, but ones that are important to your results should
be discussed (eg. don't just list them, talk about why they have that value).

You should also describe your experimental methodology; this is where you talk
about your data and what you did with it.  Talk about what sorts of experiments
you performed, and how you validated them.  For example, if you used 7-fold
cross-validation, you would say that you used it, define what it is, and discuss
how you implemented it.  It would also be good to discuss the strengths and
weaknesses of your chosen validation method; why did you pick the one you did?

\subsection*{Data Sets}
The data sets used were all obtained from \cite{ucimll}. The following sets of data were selected to test with:
\begin{description}
    \item[House Voting Data] This set contains 435 examples of votes profiles of congresspeople, and a label of their affiliation, whether it be Republican or Democrat. 16 attributes corresponding to 16 different votes by the same representative are recorded, and for each attribute, the values can be either yes, no or abstain.
    \item[Monk's Problems Data] These are 3 sets of examples of classification problems that were designed to be tough. Each problem has only 7 attributes with varying numbers of values, but the last attribute is difficult to deal with since it contains hundreds of values. In terms of classes, the 2 options are 0 and 1.
    \item[Mushroom Data] This set contains 22 attributes with varying cardinalities, but the only 2 classes are `edible' and `poisonous'. There are 8124 examples altogether.
    \item[Splice Junction] This is a particularly difficult set. 59 attributes have only the values of the possible DNA bases G, C, T and A. An additional attribute gives a specific name, making that attribute have a very large number of values. Altogether there are 3190 examples.
\end{description}

Any set that had separate training and test data had those two sections joined together, since we do k-fold cross-validation.

Every data set was modified using Vim macros and regex expressions to match the legal input of our program. In particular, the data sets were normalized to have a comma between each attribute/label. For the Splice Junction data, the examples were also pre-shuffled using the UNIX `shuf' command. This is because the classes were lumped together initially, making the test much harder than it needed to be.

\section{Results}

The results of the first experiment can be observed in table \ref{tab:fitness}. We used the same parameters (as listed in the previous section) with the exception of the fitness function. Interestingly, recall was the superior fitness function for all but the splice dataset.

\begin{table}[htbp]
\caption{Comparison of GAs with different fitness functions}
\begin{center}
\begin{tabular}{|l|l|r|r|r|r|r|r|}
\hline
\textbf{Fitness} & \textbf{Measure} & \textbf{House} & \textbf{Monks-1} & \textbf{Monks-2} & \textbf{Monks-3} & \multicolumn{1}{l|}{\textbf{Mush}} & \multicolumn{1}{l|}{\textbf{Splice}} \\ \hline
Prec & Avg & 0.93 & 0.743 & 0.464 & 0.806 & 0.752 & 0.492 \\ \cline{2-7}
 & Std Div & 0.04 & 0.17 & 0.1 & 0.164 & 0.252 & 0.068 \\ \hline
 Recall & Avg & 0.95 & 0.714 & 0.657 & 0.823 & 0.856 & 0.496 \\ \cline{2-7}
 & Std Div & 0.038 & 0.144 & 0.089 & 0.133 & 0.173 & 0.083 \\ \hline
 Mixed & Avg & 0.93 & 0.747 & 0.626 & 0.818 & 0.887 & 0.502 \\ \cline{2-7}
 & Std Div & 0.055 & 0.172 & 0.123 & 0.129 & 0.14 & 0.089 \\ \hline
\end{tabular}
\end{center}
\label{tab:fitness}
\end{table}

The second parameter with which we played is the filter probability. The results can be seen in table \ref{tab:filter}.

\begin{table}[htbp]
\caption{Comparison of GAs with different filter probabilities}
\begin{tabular}{|l|l|r|r|r|r|r|r|}
\hline
\textbf{Filter} & \textbf{Measure} & \multicolumn{1}{l|}{\textbf{House}} & \multicolumn{1}{l|}{\textbf{Monks-1}} & \multicolumn{1}{l|}{\textbf{Monks-2}} & \multicolumn{1}{l|}{\textbf{Monks-3}} & \multicolumn{1}{l|}{\textbf{Mush}} & \multicolumn{1}{l|}{\textbf{Splice}} \\ \hline
\multicolumn{1}{|r|}{0.6} & Avg & 0.9 & 0.747 & 0.466 & 0.751 & 0.777 & 0.529 \\ \cline{2-8}
& Std Div & 0.048 & 0.172 & 0.045 & 0.146 & 0.282 & 0.069 \\ \cline{2-8}
 & Time (s) & 11.9 & 25.5 & 49.1 & 23.9 & 318 & 130 \\ \hline
 \multicolumn{1}{|r|}{1} & Avg & 0.93 & 0.743 & 0.464 & 0.806 & 0.752 & 0.492 \\ \cline{2-8}
 & Std Div & 0.04 & 0.17 & 0.1 & 0.164 & 0.252 & 0.068 \\ \cline{2-8}
 & Time (s) & 0.058 & 50 & 50 & 35.2 & 532 & 234 \\ \hline
\end{tabular}
\label{tab:filter}
\end{table}

Comparing the different selection functions, we obtained the results in table \ref{tab:selection}.

\begin{table}[htbp]
\caption{Comparison of GAs with different selection functions}
\begin{center}
\begin{tabular}{|l|l|r|r|r|r|r|r|}
\hline
\textbf{Selection} & \textbf{Measure} & \multicolumn{1}{l|}{\textbf{House}} & \multicolumn{1}{l|}{\textbf{Monks-1}} & \multicolumn{1}{l|}{\textbf{Monks-2}} & \multicolumn{1}{l|}{\textbf{Monks-3}} & \multicolumn{1}{l|}{\textbf{Mush}} & \multicolumn{1}{l|}{\textbf{Splice}} \\ \hline
Fitness & Avg & 0.93 & 0.743 & 0.464 & 0.806 & 0.752 & 0.492 \\ \cline{2-8}
Proportionate & Std Div & 0.04 & 0.17 & 0.1 & 0.164 & 0.252 & 0.068 \\ \hline
Rank-based & Avg & 0.952 & 0.747 & 0.454 & 0.79 & \multicolumn{1}{l|}{-} & 0.58 \\ \cline{2-8}
 & Std Div & 0.031 & 0.172 & 0.093 & 0.125 & \multicolumn{1}{l|}{-} & 0.051 \\ \hline
 Tournament & Avg & 0.952 & 0.747 & 0.424 & 0.793 & 0.878 & 0.602 \\ \cline{2-8}
 & Std Div & 0.032 & 0.172 & 0.108 & 0.131 & 0.106 & 0.059 \\ \hline
\end{tabular}
\end{center}
\label{tab:selection}
\end{table}

We ran an experiment comparing different replacement functions, and the results can be viewed in table \ref{tab:replacement}.

\begin{table}[htbp]
\caption{Comparison of GAs with different replacement functions}
\begin{center}
\begin{tabular}{|l|l|r|r|r|r|r|r|}
\hline
\textbf{Replace fn} & \textbf{Measure} & \multicolumn{1}{l|}{\textbf{House}} & \multicolumn{1}{l|}{\textbf{Monks-1}} & \multicolumn{1}{l|}{\textbf{Monks-2}} & \multicolumn{1}{l|}{\textbf{Monks-3}} & \multicolumn{1}{l|}{\textbf{Mush}} & \multicolumn{1}{l|}{\textbf{Splice}} \\ \hline
Elitism & Avg & 0.95 & 0.742 & 0.491 & 0.768 & 0.757 & 0.502 \\ \cline{2-8}
 & Std Div & 0.03 & 0.181 & 0.123 & 0.11 & 0.226 & 0.089 \\ \hline
 Replacement & Avg & 0.93 & 0.743 & 0.464 & 0.806 & 0.752 & 0.492 \\ \cline{2-8}
 & Std Div & 0.04 & 0.17 & 0.1 & 0.164 & 0.252 & 0.068 \\ \hline
\end{tabular}
\end{center}
\label{tab:replacement}
\end{table}

We compared all possible variations of ID3 algorithms with different parameters, as can be seen in table \ref{tab:id3}.

\begin{table}[htbp]
\caption{Comparison of ID3 parameters}
\begin{tabular}{|l|l|r|r|r|r|r|r|}
\hline
\textbf{Method} & \textbf{Measure} & \multicolumn{1}{l|}{\textbf{House}} & \multicolumn{1}{l|}{\textbf{Monks-1}} & \multicolumn{1}{l|}{\textbf{Monks-2}} & \multicolumn{1}{l|}{\textbf{Monks-3}} & \multicolumn{1}{l|}{\textbf{Mush}} & \multicolumn{1}{l|}{\textbf{Splice}} \\ \hline
Information & Avg & 0.947 & 0.721 & 0.87 & 0.645 & 1 & 0.49 \\ \cline{2-8}
Gain & Std Div & 0.028 & 0.209 & 0.096 & 0.22 & 0.001 & 0.105 \\ \hline
Gain & Avg & 0.945 & 0.685 & 0.87 & 0.946 & 1 & 0.908 \\ \cline{2-8}
Ratio & Std Div & 0.026 & 0.204 & 0.096 & 0.045 & 0 & 0.008 \\ \hline
Chi-Square & Avg & 0.957 & 0.721 & 0.87 & 0.645 & 1 & 0.519 \\ \cline{2-8}
 & Std Div & 0.02 & 0.21 & 0.096 & 0.22 & 0.001 & 0.03 \\ \hline
 Chi-Square & Avg & 0.954 & 0.602 & 0.87 & 0.982 & 1 & 0.857 \\ \cline{2-8}
w/ GR & Std Div & 0.018 & 0.1 & 0.096 & 0.028 & 0 & 0.011 \\ \hline
\end{tabular}
\label{tab:id3}
\end{table}

And finally, the best GA vs. the best ID3 version:

\begin{table}[htbp]
\caption{Comparison of the best GA with the best ID3}
\begin{center}
\begin{tabular}{|l|l|r|r|r|r|r|r|}
\hline
\textbf{Replace fn} & \textbf{Measure} & \multicolumn{1}{l|}{\textbf{House}} & \multicolumn{1}{l|}{\textbf{Monks-1}} & \multicolumn{1}{l|}{\textbf{Monks-2}} & \multicolumn{1}{l|}{\textbf{Monks-3}} & \multicolumn{1}{l|}{\textbf{Mush}} & \multicolumn{1}{l|}{\textbf{Splice}} \\ \hline
ID3 & Avg & 0.945 & 0.685 & 0.87 & 0.946 & 1 & 0.908 \\ \hline
 & Std Div & 0.026 & 0.204 & 0.096 & 0.045 & 0 & 0.008 \\ \hline
 & Time & 0.062 & 0.083 & 0.125 & 0.083 & 1.08 & 5.48 \\ \hline
GA & Avg & 0.95 & 0.624 & 0.657 & 0.688 & 0.831 & 0.488 \\ \hline
 & Std Div & 0.039 & 0.2 & 0.089 & 0.117 & 0.162 & 0.114 \\ \hline
 & Time & 18.1 & 30.8 & 33.6 & 30.7 & 507 & 211 \\ \hline
\end{tabular}
\end{center}
\label{id3ga}
\end{table}

\section{Discussion}

Interestingly, the genetic algorithms perform rather poorly on most data sets. This was a complete surprise, as we had expected the effectiveness of genetic algorithms to be proportionate to their running time (or the amount of time needed to implement them properly)  --- clearly, we were mistaken. 

Table \ref{tab:fitness} indicates that different fitness functions had little effect in making the genetic algorithm perform better. It appears that recall does a little better than both precision and the mixed fitness function. It achieves better average accuracy with a lower spread. 
We see in table \ref{tab:filter} that using a filter probability (training on only a random sample of the data at each generation) doesn't seem to harm the accuracy of the algorithm by much, if at all. In fact, some data sets did better with a filter. More significantly, the time taken to perform the algorithm is greatly reduced for large data sets (such as the muchroom data -- a 40\% reduction!).  
From the results in table \ref{tab:selction} it appears that rank-based selection does better on these data sets than other selection criteria, but only slightly so. Different replacement functions also seem to have very little effect on accuracy, though replacement has a slight edge over elitism in our tests.

On the ID3 side, in table \ref{tab:id3} it appears that no parameter covers every data set. Every algorithm variation has done well with the House data set, but the Monks data sets are clearly more challenging. However, while the GAs struggle with the Monks-3 dataset, the Gain-Ratio variations of ID3 do very well with it, probably because gain ratio eliminates the dominance of the last attribute in that set, which contains many values. Interestingly, all ID3 algorithms do spectacularly well with the Mushroom set, while the GR-based versions even do well on the splice set. Intresetingly enough, it appears that for these data sets, the non-Chi-Square GR version is the most effective.

Looking at table \ref{id3ga}, we see that not only does the ID3 algorithm beat GAs in almost all data sets, it also trounces it in time elapsed. We see no reason to recommend GAs for most decision-tree based tasks when ID3 with its different variations is so much faster and better.

We're not sure why GAs underperformed as much as they did. Perhaps the mutation component operator is too weak on decision trees due to their constraints, or perhaps our fitness function is deficient. One of the problems with GAs is that you don't know if the problem is with the algorithm itself or with some parameters.


\section{Conclusions}
From our investigation and comparison of genetic algorithms for decision trees vs ID3, we conclude that in almost all cases where decision trees are applicable, ID3 does a good job of classifying data, and it does it much faster than genetic algorithms. As is often the case in machine learning, a good heuristic can beat random exploration of the data. It's possible that under some circumstances, genetic algorithms will be better than ID3. We have not found that to be the case with our data.

We hope to study this topic further, specifically the underperformance of GAs. We hope to find some data sets on which they prove to be more effective.

% many different styles of bibliography are available; plain is fine for this
% assignment
\bibliographystyle{plain}

% the bibliography command should contain the name of your .bib file, minus the
% extension.
\bibliography{hw4bib}
%\printbibliography

% because "document" is an environment, you need to have a closing tag at the
% end of your document.  Anything written after this tag will not be included in
% the generated output.
\end{document}
